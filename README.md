# Heart_Stroke

### Preface

	This project follows a specific pattern to ensure that anyone who reads it, can understand each and every step in a better way. 
Our inspiration and backbone for this project to happen, is our Suresh Sir                         (Dr. Suresh R, Assistant Professor, Bangalore University). The way he thought us the concepts of Machine learning, inspired us to implement it in our project. 
This project is intended for those who are interested in Machine Learning. Note that the data is to be converted to that format, to which the model can be fitted. As, the Machine Learning model cannot be fitted to the data in any format. We also need to keep in mind that the data should not contain missing observations.
Coming to Machine Learning, Arthur Samuel, an early American leader in the field of computer gaming and artificial intelligence, coined the term “Machine Learning” in 1959 while at IBM. He defined machine learning as “the field of study that gives computers the ability to learn without being explicitly programmed “. However, different authors define the term differently. The further explanation related to machine learning will be found from chapter 5.0.
Machine learning is a subfield of artificial intelligence that involves the development of algorithms and statistical models that enable computers to improve their performance in tasks through experience. These algorithms and models are designed to learn from data and make predictions or decisions without explicit instructions. There are several types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on labeled data, while unsupervised learning involves training a model on unlabeled data. Reinforcement learning involves training a model through trial and error. Machine learning is used in a wide variety of applications, including image and speech recognition, natural language processing, and recommender systems.
For more details related to Machine Learning, refer https://www.geeksforgeeks.org/machine-learning/?ref=lbp or https://www.javatpoint.com/machine-learning. 


1.0 HEART STROKE

1.1 INTRODUCTION

 
How would you react to a medical emergency? When it comes to life-threatening conditions like heart attack or stroke, every minute counts. Get to know the signs and symptoms of these health threats. If you think you or someone else might be having a heart attack or stroke, get medical help right away. Acting fast could save your life or someone else’s.
Heart disease and stroke are 2 of the top killers among both women and men in the U.S. Nationwide, someone dies from a heart attack about every 90 seconds, and stroke kills someone about every 4 minutes, according to the U.S. Centers for Disease Control and Prevention. Quick medical help could prevent many of these deaths. Fast action can also limit permanent damage to the body.
Heart attack and stroke are caused by interruptions to the normal flow of blood to the heart or brain—2 organs that are essential to life. Without access to oxygen-rich blood and nutrients, heart or brain cells begin to malfunction and die. This cell death can set off a series of harmful effects throughout the body. The changes ultimately lead to the familiar symptoms of a heart or brain emergency.
You might know the most common symptoms of heart attack: sustained, crushing chest pain and difficulty breathing. A heart attack might also cause cold sweats, a racing heart, pain down the left arm, jaw stiffness, or shoulder pain.
Many don’t know that women often have different heart attack symptoms than men. For instance, instead of having chest pain during a heart attack, women may feel extremely exhausted and fatigued or have indigestion and nausea.
“Many women have a vague sense of gloom and doom, a sense of ‘I just don’t feel quite right and don’t know why,’ ” says Dr. Patrice Desvigne-Nickens, an NIH expert in heart health.
The symptoms of stroke include sudden difficulty seeing, speaking, or walking, and feelings of weakness, numbness, dizziness, and confusion. “Some people get a severe headache that’s immediate and strong, different from any kind you’ve ever had,” says Dr. Salina Waddy, an NIH stroke expert.
At the first sign of any of these symptoms, fast action by you, someone you know, or a passer-by can make a huge difference. NIH-funded research has helped ensure that more people survive heart attacks and strokes every year. We now have medicines, procedures, and devices that can help limit heart and brain damage following an attack, as long as medical help arrives quickly.
If the heart is starved for blood for too long—generally more than 20 minutes—heart muscle can be irreversibly damaged, Desvigne-Nickens says. “You need to be in the hospital because there’s a risk of cardiac arrest [your heart stopping],” which could be deadly. At the hospital, doctors can administer clot-busting drugs and other emergency procedures.
With stroke, Waddy says, “The longer you wait, the more brain cells are dying,” and the greater the chance for permanent damage or disability.
Emergency treatment for stroke depends on the kind of stroke. The most common type, ischemic stroke, is caused by a clot that clogs a blood vessel in the brain. The clot-dissolving drug tPA works best when given soon after symptoms begin. NIH research shows that patients who received tPA within 3 hours of stroke onset were more likely to recover fully.
Other strokes are caused by a haemorrhage—when a blood vessel breaks and bleeds into the brain. “The patient can have a larger haemorrhage within the first 3 hours,” Waddy says. A hospital medical team can help contain the bleeding, so every moment counts.
Even if you’re unsure, don’t feel embarrassed or hesitate to call 9-1-1 if you suspect a heart attack or stroke. “You should not go get your car keys. Your spouse shouldn’t be driving you to the hospital,” advises Desvigne-Nickens. “The emergency crew is trained to treat these symptoms, and it could mean the difference between life and death.”
Heart attack or stroke can happen to anyone, but your risk increases with age. A family or personal history of heart attack or stroke also raises your risk. But some risk factors for heart attack and stroke are within your control. Treating them can dramatically reduce your risk.
“If you have high blood pressure, high cholesterol, or diabetes, work with your doctor to get these conditions under control,” Waddy says. “Know your numbers [blood pressure, blood sugar, and cholesterol] and what they mean.”
You can also prepare for a medical emergency, to some degree. A hospital may not have access to your medical records when you arrive. Keep important health information handy, such as the medicines you’re taking, allergies, and emergency contacts. It would be important for the medical team to know, for example, if you’ve been taking anticoagulants to help prevent blood clots; these blood thinners put you at increased risk of bleeding. You might consider carrying an NIH wallet_card that lists heart attack symptoms and has room for your personal medical information.
NIH researchers are studying new drugs and procedures to help the heart and brain repair themselves and improve organ function. “But there is absolutely nothing that will save both your time and health as well as prevention,” says Dr. Jeremy Brown, director of NIH’s Office of Emergency Care Research. Studies show that making healthy lifestyle choices can help prevent these medical emergencies from happening in the first place. Eat a healthy diet rich in protein, whole grains, and fruits and vegetables, and low in saturated fat. Get regular physical activity and don’t smoke.
“I think one of the most important things we can do is to take a basic CPR and first aid course,” recommends Brown. “We know the majority of cardiac arrests happen outside of hospitals and of that many, many can be saved if we get people with basic training on the scene quickly. An ambulance can never get there as quickly as a citizen passing by.”
Whether or not you’re trained to offer help, if you see someone having symptoms of a heart attack or stroke, call for help immediately.
1.2 Symptoms of Heart Stroke
The symptoms of Heart Stroke are as follows.
Heart attack:
	Chest pain or discomfort
	Pain, stiffness, or numbness in the neck, back, or one or both arms or shoulders
	Shortness of breath
	Cold sweat, nausea, dizziness
Stroke:
	Sudden numbness or weakness of the face, arm, or leg, especially on one side of the body
	Sudden severe headache, dizziness, confusion
	Sudden difficulty with vision, balance, speech
1.3 Types of cardiovascular disease
Cardiovascular disease covers a number of conditions that are related to lifestyle, including:
	coronary heart disease – either angina or heart attack (acute myocardial infarction)
	stroke – either caused by a blockage with a blood clot (called an ischaemic stroke) or the rupturing of a blood vessel and bleeding (called a haemorrhagic stroke)
	peripheral vascular disease – obstruction of the large blood vessels that supply blood to the arms and legs.
Cardiovascular disease conditions that are not related to lifestyle, include:
	Acute rheumatic fever and rheumatic heart disease – caused by an untreated infection with group A streptococcus bacteria
	Congenital heart disease – inherited conditions that affect the structure (such as valves) of the heart.
By far the biggest cause of deaths from cardiovascular disease is the progressive blocking of blood vessels leading to coronary heart disease and stroke.
1.4 Causes of coronary heart disease and stroke
Healthy blood vessels are flexible, but with age and unhealthy lifestyle choices, they can become thickened and stiff, and this can restrict blood flow around the body. This process is known as arteriosclerosis and is commonly called ‘hardenings of the arteries.
Atherosclerosis is a form of arteriosclerosis that involves a build-up of fatty substances and cellular waste (plaques). These can either partially or totally block blood vessels, or the plaque can break open and trigger a blood clot that also blocks blood flow. Atherosclerosis can occur anywhere in the body. For example, when it occurs in the vessels leading to your arms and legs, it can cause peripheral vascular disease (PVD).
When the process of atherosclerosis occurs in the arteries that supply blood to your heart muscle (coronary heart disease or CHD), it can trigger angina or a heart attack.
When this process occurs in arteries supplying blood to the brain, the arteries become narrow with plaques, and a blood clot can form and block the blood supply to the brain (thrombotic stroke). In other cases, a blood clot may travel from elsewhere in the body (such as the heart) and lodge in the narrowed arteries (embolic stroke).
Thrombotic stroke and embolic stroke are both causes of the most common type of stroke, ischaemic stroke. Haemorrhagic stroke, which is less common, occurs when a blood vessel in the brain breaks and bleeds.
Although blocked blood vessels can cause both coronary heart disease and some types of stroke, stroke is not the same as heart disease.
1.5 Reduce the risk of coronary heart disease and stroke
You can reduce the risk of cardiovascular disease, which kills nearly one third of Australians, by making healthy lifestyle choices such as:
	Avoid smoke or smoking – smoking is a major risk factor, with nicotine directly narrowing your blood vessels.
	Eat healthy foods – especially avoid processed foods.
	Exercise and move more – speak to your doctor about exercise suitable for your needs.
	Maintain a healthy weight – exercising and eating healthy food will make this easier.
	Manage stress – try muscle relaxation, breathing techniques or visualisations.
Visit your doctor to discuss lifestyle choices that will reduce your risk of cardiovascular disease. You can also ask for advice about regular check-ups to look for early signs of medical conditions such as:
	depression
	diabetes
	high blood pressure
	high cholesterol.

1.6 Data source
It is a Secondary data that has been taken from Kaggle website.
Website: https://www.kaggle.com/datasets/captainozlem/framingham-chd-preprocessed-data
Sample Data
 



1.7 Attributes 
	sex: male or female (Binary)
	age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)
	education: 0: Less than High School and High School degrees, 1: College Degree and Higher
	 current Smoker: whether or not the patient is a current smoker (Binary) 
	cigsPerDay: the number of cigarettes that the person smoked on average in one day (can be considered continuous as one can have any number of cigarettes, even half a cigarette).
	BPMeds: whether or not the patient was on blood pressure medication (Binary)
	prevalentStroke: whether or not the patient had previously had a stroke (Binary)
	prevalentHyp: whether or not the patient was hypertensive (Binary)
	diabetes: whether or not the patient had diabetes (Binary)
	totChol: total cholesterol level (Continuous)
	sysBP: systolic blood pressure (Continuous)
	diaBP: diastolic blood pressure (Continuous)
	BMI: Body Mass Index (Continuous)
	heartRate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)
	glucose: glucose level (Continuous).
	HeartStroke: 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)

1.8 Objectives
	The main objectictives of this project are listed below.
	To study the presence of outliers (if present) on the basis of correlation matrix.
	To develop suitable Machine Learning Model for the data.
	Checking the performance of the model with the help of AUC and ROC curve.



Software Used
	Python




1.9 Contingency Table

	BP Medication and Heart Stroke
Input: (The below command gives the contingency table for BP Medication and Heart Stroke)
cross_tab_1=pd.crosstab(heart['BPMeds'],heart['HeartStroke'])
print(cross_tab_1)

Output:
Heart Stroke	0	1
BP Meds		
0	3410	581
1	95	47

	Prevalent stroke and Heart Stroke
Input: (The below command gives the contingency table for Prevalent Stroke and Heart Stroke)
cross_tab_2=pd.crosstab(heart['prevalentStroke'],heart['HeartStroke'])
cross_tab_2


Output:
Heart Stroke	0	1
Prevalent Stroke		
0	3491	617
1	14	11

	Prevalent Hypertensive and Heart Stroke
Input: (The below command gives the contingency table for Prevalent Hypertensive and Heart Stroke)
cross_tab_3=pd.crosstab(heart['prevalentHyp'],heart['HeartStroke'])
cross_tab_3

Output:
Heart Stroke	0	1
Prevalent Hyp		
0	2538	309
1	967	319

	Diabetes and Heart Stroke
Input: (The below command gives the contingency table for Diabetes and Heart Stroke)
cross_tab_4=pd.crosstab(heart['diabetes'],heart['HeartStroke'])
cross_tab_4

Output:
Heart Stroke	0	1
Diabetes		
0	3438	589
1	67	39






















2.0 Data Pre-processing in Machine learning
Data pre-processing is a process of preparing the raw data and making it suitable for a machine learning model. It is the first and crucial step while creating a machine learning model. 
When creating a machine learning project, it is not always a case that we come across the clean and formatted data. And while doing any operation with data, it is mandatory to clean it and put in a formatted way. So for this, we use data pre-processing task.
2.1 Why do we need Data Pre-processing?
A real-world data generally contains noises, missing values, and maybe in an unusable format which cannot be directly used for machine learning models. Data pre-processing is required tasks for cleaning the data and making it suitable for a machine learning model which also increases the accuracy and efficiency of a machine learning model. 

It steps involved are as follows:
	Getting the dataset
	Importing libraries
	Importing datasets
	Finding Missing Data
	Outlier detection
	Feature scaling
	Splitting dataset into training and test set
	Fitting the model
	Checking the performance of the model
2.1.1 Get the Dataset
To create a machine learning model, the first thing we required is a dataset, as a machine learning model completely works on data. The collected data for a particular problem in a proper format is known as the dataset.
Dataset may be of different formats for different purposes, such as, if we want to create a machine learning model for business purpose, then dataset will be different with the dataset required for a liver patient. So each dataset is different from another dataset. To use the dataset in our code, we usually put it into a CSV file. However, sometimes, we may also need to use an HTML or xlsx file. 
What is a CSV File?
CSV stands for "Comma-Separated Values" files; it is a file format which allows us to save the tabular data, such as spreadsheets. It is useful for huge datasets and can use these datasets in programs. 
2.2 Importing Libraries
In order to perform data pre-processing using Python, we need to import some predefined Python libraries. These libraries are used to perform some specific jobs. There are three specific libraries that we will use for data pre-processing, which are:
They are imported as below:
import pandas as pd (import data, convert the data into data frame and to form a table)
import matplotlib.pyplot as plt (Graphical representation)
import seaborn as sns (Graphical representation)
from scipy import stats
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler (Normalising the data)
import xgboost as xg
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score


2.3 Importing the Datasets
Now we need to import the datasets which we have collected for our machine learning project.
Here, we are naming the data as heart.
Input:
heart = pd.read_csv(r"D:\Suhani\CHD_preprocessed.csv")
print(heart)


Output:
 


2.3.1 Extracting dependent and independent variables:
	In machine learning, it is important to distinguish the matrix of features (independent variables) and dependent variables from dataset. In our dataset, there are three independent variables that are gender, age, education, currentSmoker, cigsPerDay, BPMeds, prevalentStroke, prevalentHyp, diabetes, totChol, sysBP, diaBP, BMI, heartRate and glucose, and one is a dependent variable which is HeartStroke. 
	Feature selection from the independent variables is done with the help of correlation matrix, i.e, if higher multicollinearity exists between two features, then one of that feature is removed from the data used for model building.
	In this case, since outliers exists, we cannot use correlation matrix for feature selection. The concept ‘Outliers’ is explained further.

Multicolliniarity and Correlation

	Multicolliniarrity is checked with the help of correlation matrix.

Correlation matrix: A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis.
Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate).
	Correlation is denoted as r, which ranges from -1 to +1. 
	The closer r is to zero, the weaker the linear relationship.
	Positive r values indicate a positive correlation, where the values of both variables tend to increase together.
	Negative r values indicate a negative correlation, where the values of one variable tend to increase when the values of the other variable decrease.
In general, if x & y are the two variables of discussion, then the correlation coefficient can be calculated using the formula,
 
 Where, 
	rxy is the correlation coefficient of the linear relationship between the variables x and y 
	xi ‘s are the values of the x-variable in a sample 
	x̅ is the mean of the values of the x-variable 
	yi ‘s are the values of the y-variable in a sample

Multicolliniarity: It occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.
	Multicollinearity generates high variance of the estimated coefficients and hence, the coefficient estimates corresponding to those interrelated explanatory variables will not be accurate in giving us the actual picture. They can become very sensitive to small changes in the model.
	It is also possible that the adjusted R squared for a model is pretty good and even the overall F-test statistic is also significant but some of the individual coefficients are statistically insignificant. This scenario can be a possible indication of the presence of multicollinearity as multicollinearity affects the coefficients and corresponding p-values, but it does not affect the goodness-of-fit statistics or the overall model significance.

Correlation matrix is shown in the form of heat map:

Input:
plt.figure(figsize=(13,13))
sns.heatmap(heart.corr(),cmap='Blues',annot=True)
plt.show()













Output:

 

	From the Correlation matrix, it is clear that higher multicolliniarity exists between ‘sysBP’ and ‘prevalentHyp’, ‘diaBP’ and ‘prevalentHyp’, ‘glucose’ and ‘diabetes’, ‘currentSmoker’ and ‘cigsPerDay’. Hence we are not using ‘sysBP’, ‘diaBP’, glucose and ‘cigsPerDay’ for building the model.
	But, the above matrix is not enough for feature selection. Thus we go for Chi_square test.





Chi-Square Test for feature selection
	A chi-square test is used in statistics to test the independence of two events. Given the data of two variables, we can get observed count O and expected count E. Chi-Square measures how expected count E and observed count O deviates each other.
 
	Steps to perform the Chi-Square Test:
	Define Hypothesis.
	Build a Contingency table.
	Find the expected values.
	Calculate the Chi-Square statistic.
	Accept or Reject the Null Hypothesis.
Input:
contingency_table=pd.crosstab(heart["education"],heart["HeartStroke"])
print('contingency_table :-\n',contingency_table)#Observed Values
Observed_Values = contingency_table.values 
print("Observed Values :-\n",Observed_Values)
b=stats.chi2_contingency(contingency_table)
Expected_Values = b[3]
print("Expected Values :-\n",Expected_Values)
no_of_rows=len(contingency_table.iloc[0:,0])
no_of_columns=len(contingency_table.iloc[0,0:])
ddof=(no_of_rows-1)*(no_of_columns-1)
print("Degree of Freedom:-",ddof)
alpha = 0.05 
from scipy.stats import chi2
chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])
chi_square_statistic=chi_square[0]+chi_square[1]
print("chi-square statistic:-",chi_square_statistic)
critical_value=chi2.ppf(q=1-alpha,df=ddof)
print('critical_value:',critical_value)#p-value
p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)
print('p-value:',p_value)
print('Significance level: ',alpha)
print('Degree of Freedom: ',ddof)
print('chi-square statistic:',chi_square_statistic)
print('critical_value:',critical_value)
print('p-value:',p_value)
if chi_square_statistic>=critical_value:
    print("Reject H0,There is a relationship between 2 categorical variables")
else:
    print("Retain H0,There is no relationship between 2 categorical variables")
    
if p_value<=alpha:
    print("Reject H0,There is a relationship between 2 categorical variables")
else:
    print("Retain H0,There is no relationship between 2 categorical variables")






Output:
contingency_table :-
 HeartStroke     0    1
education             
0            2503  470
1            1002  158
Observed Values :-
 [[2503  470]
 [1002  158]]
Expected Values :-
 [[2521.25937576  451.74062424]
 [ 983.74062424  176.25937576]]
Degree of Freedom:- 1
chi-square statistic:- 3.100755361593883
critical_value: 3.841458820694124
p-value: 0.07825597637337878
Significance level:  0.05
Degree of Freedom:  1
chi-square statistic: 3.100755361593883
critical_value: 3.841458820694124
p-value: 0.07825597637337878
Retain H0,There is no relationship between 2 categorical variables
Retain H0,There is no relationship between 2 categorical variables


	Thus from the Chi-square test, there is no relationship between education and ‘HeartStroke’. As a result, we are not using the independent feature ‘education’ also for model building.
	

Feature Selection
Input: 
X = heart.drop(["HeartStroke","education","currentSmoker",'glucose','sysBP','diaBP'],axis=1)
y = heart["HeartStroke"]

Note: 
	Before checking the correlation between the variables, it is very import to check whether there is/are any missing observation(s) in the data.
	Outliers effect the correlation matrix because correlation includes mean in it.
	Dependency between the variables exists even when the correlation between those two variables is equal to zero.


 2.4 Check for the Missing Data
The next step of data pre-processing is to handle missing data in the datasets. If our dataset contains some missing data, then it may create a huge problem for our machine learning model. Hence it is necessary to handle missing values present in the dataset.
Ways to handle missing data:
There are mainly two ways to handle missing data, which are:
	By deleting the particular row: The first way is used to commonly deal with null values. In this way, we just delete the specific row or column which consists of null values. But this way is not so efficient and removing data may lead to loss of information which will not give the accurate output.
	By calculating the mean: In this way, we will calculate the mean of that column or row which contains any missing value and will put it on the place of missing value. This strategy is useful for the features which have numeric data such as age, salary, year, etc. Here, we will use this approach. 

Input:
heart.isnull().sum()


Output:
gender             0
age                0
education          0
currentSmoker      0
cigsPerDay         0
BPMeds             0
prevalentStroke    0
prevalentHyp       0
diabetes           0
totChol            0
sysBP              0
diaBP              0
BMI                0
heartRate          0
glucose            0
HeartStroke        0
dtype: int64

Conclusion: From the above output it is clear that there is no missing observation in the data.




	Outlier Detection:

Outlier detection is the process of detecting outliers, or a data point that is far away from the average, and depending on what you are trying to accomplish, potentially removing or resolving them from the analysis to prevent any potential skewing.
Outliers are the data points that are far from other data points.
Some main causes of outliers are:
• In-correct data entry by humans 
• Codes used instead of values 
• Sampling errors, or data has been extracted from the wrong place or mixed with other data 
• Unexpected distribution of variables
Some methods to identify outliers:
	Graphical method: Box plot, scatter plot and histogram,
	Z – scores,
	Inter Quartile Range (IQR) to create outlier fences,
	Hypothesis test.
Box plot: It is a type of chart that depicts a group of numerical data through their quartiles. It is a simple way to visualize the shape of our data.
Components of Box plot:
	Minimum – It is the minimum value in the dataset excluding the outliers
	First Quartile (Q1) – 25% of the data lies below the First (lower) Quartile.
	Median (Q2) – It is the mid-point of the dataset. Half of the values lie below it and half above.
	Third Quartile (Q3) – 75% of the data lies below the Third (Upper) Quartile.
	Maximum – It is the maximum value in the dataset excluding the outliers.

 
Any data point that falls outside the lines (Min, Max) indicates an outlier.
Input:
data = heart
plt.figure(figsize=(15,15))

plt.subplot(5,2,1)
plt.boxplot(data["age"])
plt.title("age")

plt.subplot(5,2,2)
plt.boxplot(data["cigsPerDay"])
plt.title("cigsPerDay")

plt.subplot(5,2,3)
plt.boxplot(data["BPMeds"])
plt.title("BPMeds")

plt.subplot(5,2,4)
plt.boxplot(data["prevalentStroke"])
plt.title("prevalentStroke")

plt.subplot(5,2,5)
plt.boxplot(data["prevalentHyp"])
plt.title("prevalentHyp")

plt.subplot(5,2,6)
plt.boxplot(data["diabetes"])
plt.title("diabetes")

plt.subplot(5,2,7)
plt.boxplot(data["totChol"])
plt.title("totChol")

plt.subplot(5,2,8)
plt.boxplot(data["BMI"])
plt.title("BMI")

plt.subplot(5,2,9)
plt.boxplot(data["gender"])
plt.title("gender")

plt.subplot(5,2,10)
plt.boxplot(data["heartRate"])
plt.title("heartRate")

plt.show()





Output: 
 
Conclusion: From the above box plots, it is clear that the variables ‘cigsPerDay’, ‘totChol’, ‘BMI’,‘gender’ and ‘heartRate’ has multiple outliers.
	Hence, an outlier decreases the value of a correlation coefficient and weakens the regression relationship, but it's also possible that in some circumstances an outlier may increase a correlation value and improve regression. As a result, correlation matrix is not trust worthy, in the presence of outliers.

2.6 Feature Scaling
Feature scaling is a data pre-processing technique used to transform the values of features or variables in a dataset to a similar scale. The purpose is to ensure that all features contribute equally to the model and to avoid the domination of features with larger values.
Feature scaling becomes necessary when dealing with datasets containing features that have different ranges, units of measurement, or orders of magnitude. In such cases, the variation in feature values can lead to biased model performance or difficulties during the learning process.
There are several common techniques for feature scaling, including standardization, and min-max scaling. These methods adjust the feature values while preserving their relative relationships and distributions.
By applying feature scaling, the dataset’s features can be transformed to a more consistent scale, making it easier to build accurate and effective machine learning models. Scaling facilitates meaningful comparisons between features, improves model convergence, and prevents certain features from overshadowing others based solely on their magnitude.
1) Normalization (Min-Max scaling): Normalization is a data preprocessing technique used to adjust the values of features in a dataset to a common scale. This is done to facilitate data analysis and modeling, and to reduce the impact of different scales on the accuracy of machine learning models.
Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.
Here’s the formula for normalization:
 
Here, Xmax and Xmin are the maximum and the minimum values of the feature, respectively.
	When the value of X is the minimum value in the column, the numerator will be 0, and hence X’ is 0.
	On the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator, and thus the value of X’ is 1.
	If the value of X is between the minimum and the maximum value, then the value of X’ is between 0 and 1.
 2) Standardization: Standardization is another scaling method where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero, and the resultant distribution has a unit standard deviation.
Here’s the formula for standardization:
 
	is the mean of the feature values and 
  is the standard deviation of the feature values. 
Note that, in this case, the values are not restricted to a particular range.

In General:
Normalization	Standardization
Rescales values to a range between 0 and 1.	Centres data around the mean and scales to a standard deviation of 1.
Useful when the distribution of the data is unknown or not Gaussian.	Useful when the distribution of the data is Gaussian or unknown.
Sensitive to outliers.	Less sensitive to outliers.
Retains the shape of the original distribution.	Changes the shape of the original distribution.
May not preserve the relationships between the data points.	Preserves the relationships between the data points.
Equation: (x – min)/(max – min)	Equation: (x – mean)/standard deviation

However, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data.
In this case, we have standardizes the data.
Input:
std = StandardScaler()
X = std.fit_transform(X)
print(X)
Output: 
[[ 1.15772065 -1.233235   -0.76375118 ... -0.94899157  0.29245652
   0.3382184 ]
 [-0.86376623 -0.4155346  -0.76375118 ...  0.3037452   0.72447848
   1.58326617]
 [ 1.15772065 -0.18190591  0.91452387 ...  0.18986004 -0.10765474
  -0.07679753]
 ...
 [-0.86376623 -0.18190591  0.91452387 ...  0.25819113 -0.92751461
   0.67023114]
 [-0.86376623 -0.64916328  0.49495511 ... -0.60733609 -1.62464097
   0.83623751]
 [-0.86376623  0.28535147 -0.76375118 ...  0.73650881 -1.05761214
   0.3382184 ]]

Conclusion: Standardisation is done in order to give equal importance for each of the variable and to normalise the data.
Plotting the standardised data in the form of histogram.
Input:
plt.figure(figsize=(7,7))
plt.hist(X_std)
plt.show()

Output:
 














	Modelling


3.1 Splitting dataset into training and test set

In machine learning projects, we generally divide the original dataset into training data and test data. We train our model over a subset of the original dataset, i.e., the training dataset, and then evaluate whether it can generalize well to the new or unseen dataset or test set. Therefore, train and test datasets are the two key concepts of machine learning, where the training dataset is used to fit the model, and the test dataset is used to evaluate the model.
The training data is the biggest (in -size) subset of the original dataset, which is used to train or fit the machine learning model. Firstly, the training data is fed to the ML algorithms, which lets them learn how to make predictions for the given task.
Once we train the model with the training dataset, it's time to test the model with the test dataset. This dataset evaluates the performance of the model and ensures that the model can generalize well with the new or unseen dataset. The test dataset is another subset of original data, which is independent of the training dataset. However, it has some similar types of features and class probability distribution and uses it as a benchmark for model evaluation once the model training is completed. Test data is a well-organized dataset that contains data for each type of scenario for a given problem that the model would be facing when used in the real world. Usually, the test dataset is approximately 20-25% of the total original data for an ML project.
NOTE: At this stage, we can also check and compare the testing accuracy with the training accuracy, which means how accurate our model is with the test dataset against the training dataset. If it performs well with the training data, but does not perform well with the test dataset, then it is estimated that the model may be overfitted. 
Therefore, if we train and test the model with two different datasets, then it will decrease the performance of the model. Hence it is important to split a dataset into two parts, i.e., train and test set.
In this way, we can easily evaluate the performance of our model. 
Input:
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=21, test_size=0.4)

3.2 Models used
We have used three ML algorithms and they are:
	Logistic Regression
	XG Boost
	Random Forest

























4.0 Machine Learning Algorithms

4.1 Introduction to Machine Learning
In the real world, we are surrounded by humans who can learn everything from their experiences with their learning capability, and we have computers or machines which work on our instructions. But can a machine also learn from experiences or past data like a human does? So here comes the role of Machine Learning.
A subset of artificial intelligence known as machine learning focuses primarily on the creation of algorithms that enable a computer to independently learn from data and previous experiences. Arthur Samuel first used the term "machine learning" in 1959. It could be summarized as follows:
Without being explicitly programmed, machine learning enables a machine to automatically learn from data, improve performance from experiences, and predict things.
Machine learning algorithms create a mathematical model that, without being explicitly programmed, aids in making predictions or decisions with the assistance of sample historical data, or training data. For the purpose of developing predictive models, machine learning brings together statistics and computer science. Algorithms that learn from historical data are either constructed or utilized in machine learning. The performance will rise in proportion to the quantity of information we provide.
A machine can learn if it can gain more data to improve its performance.
How does Machine Learning work
A machine learning system builds prediction models, learns from previous data, and predicts the output of new data whenever it receives it. The amount of data helps to build a better model that accurately predicts the output, which in turn affects the accuracy of the predicted output.
Let's say we have a complex problem in which we need to make predictions. Instead of writing code, we just need to feed the data to generic algorithms, which build the logic based on the data and predict the output. Our perspective on the issue has changed as a result of machine learning. The Machine Learning algorithm's operation is depicted in the following block diagram:
 
Features of Machine Learning:
	Machine learning uses data to detect various patterns in a given dataset.
	It can learn from past data and improve automatically.
	It is a data-driven technology.
	Machine learning is much similar to data mining as it also deals with the huge amount of the data.
Need for Machine Learning
The demand for machine learning is steadily rising. Because it is able to perform tasks that are too complex for a person to directly implement, machine learning is required. Humans are constrained by our inability to manually access vast amounts of data; as a result, we require computer systems, which is where machine learning comes in to simplify our lives.
By providing them with a large amount of data and allowing them to automatically explore the data, build models, and predict the required output, we can train machine learning algorithms. The cost function can be used to determine the amount of data and the machine learning algorithm's performance. We can save both time and money by using machine learning.
The significance of AI can be handily perceived by its utilization's cases, Presently, AI is utilized in self-driving vehicles, digital misrepresentation identification, face acknowledgment, and companion idea by Facebook, and so on. Different top organizations, for example, Netflix and Amazon have constructed AI models that are utilizing an immense measure of information to examine the client interest and suggest item likewise.
Following are some key points which show the importance of Machine Learning:
	Rapid increment in the production of data
	Solving complex problems, which are difficult for a human
	Decision making in various sector including finance
	Finding hidden patterns and extracting useful information from data.

4.2 Classification of Machine Learning
At a broad level, machine learning can be classified into three types:
	Supervised learning
	Unsupervised learning
	Reinforcement learning
 

1) Supervised Learning
In supervised learning, sample labeled data are provided to the machine learning system for training, and the system then predicts the output based on the training data.
The system uses labeled data to build a model that understands the datasets and learns about each one. After the training and processing are done, we test the model with sample data to see if it can accurately predict the output.
The mapping of the input data to the output data is the objective of supervised learning. The managed learning depends on oversight, and it is equivalent to when an understudy learns things in the management of the educator. Spam filtering is an example of supervised learning.
Supervised learning can be grouped further in two categories of algorithms:
	Classification
	Regression
2) Unsupervised Learning
Unsupervised learning is a learning method in which a machine learns without any supervision.
The training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision. The goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns.
In unsupervised learning, we don't have a predetermined result. The machine tries to find useful insights from the huge amount of data. It can be further classifieds into two categories of algorithms:
	Clustering
	Association
3) Reinforcement Learning
Reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action. The agent learns automatically with these feedbacks and improves its performance. In reinforcement learning, the agent interacts with the environment and explores it. The goal of an agent is to get the most reward points, and hence, it improves its performance.
The robotic dog, which automatically learns the movement of his arms, is an example of Reinforcement learning.
4.3 Regression Analysis in Machine learning
Regression analysis is a statistical method to model the relationship between a dependent (target) and independent (predictor) variables with one or more independent variables. More specifically, Regression analysis helps us to understand how the value of the dependent variable is changing corresponding to an independent variable when other independent variables are held fixed. It predicts continuous/real values such as temperature, age, salary, price, etc.
Terminologies Related to the Regression Analysis:
	Dependent Variable: The main factor in Regression analysis which we want to predict or understand is called the dependent variable. It is also called target variable.
	Independent Variable: The factors which affect the dependent variables or which are used to predict the values of the dependent variables are called independent variable, also called as a predictor.
	Outliers: Outlier is an observation which contains either very low value or very high value in comparison to other observed values. An outlier may hamper the result, so it should be avoided.
	Multicollinearity: If the independent variables are highly correlated with each other than other variables, then such condition is called Multicollinearity. It should not be present in the dataset, because it creates problem while ranking the most affecting variable.
	Underfitting and Overfitting: If our algorithm works well with the training dataset but not well with test dataset, then such problem is called Overfitting. And if our algorithm does not perform well even with training dataset, then such problem is called underfitting.
Why do we use Regression Analysis?
As mentioned above, Regression analysis helps in the prediction of a continuous variable. There are various scenarios in the real world where we need some future predictions such as weather condition, sales prediction, marketing trends, etc., for such case we need some technology which can make predictions more accurately. So for such case we need Regression analysis which is a statistical method and used in machine learning and data science. Below are some other reasons for using Regression analysis:
	Regression estimates the relationship between the target and the independent variable.
	It is used to find the trends in data.
	It helps to predict real/continuous values.
	By performing the regression, we can confidently determine the most important factor, the least important factor, and how each factor is affecting the other factors.
Types of Regression
There are various types of regressions which are used in data science and machine learning. Each type has its own importance on different scenarios, but at the core, all the regression methods analyse the effect of the independent variable on dependent variables. Here we are discussing some important types of regression which are given below:
	Linear Regression
	Polynomial Regression
	Support Vector Regression
	Decision Tree Regression
	Random Forest Regression
	Ridge Regression
	LASSO Regression

4.4 Classification Algorithm in Machine Learning 
As we know, the Supervised Machine Learning algorithm can be broadly classified into Regression and Classification Algorithms. In Regression algorithms, we have predicted the output for continuous values, but to predict the categorical values, we need Classification algorithms. 
4.4.1 What is the Classification Algorithm?
The Classification algorithm is a Supervised Learning technique that is used to identify the category of new observations on the basis of training data. In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups. Such as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc. Classes can be called as targets/labels or categories.
Unlike regression, the output variable of Classification is a category, not a value, such as "Green or Blue", "fruit or animal", etc. Since the Classification algorithm is a Supervised learning technique, hence it takes labeled input data, which means it contains input with the corresponding output. 
In classification algorithm, a discrete output function(y) is mapped to input variable(x).
	y=f(x), where y = categorical output  
The best example of an ML classification algorithm is Email Spam Detector. 
The main goal of the Classification algorithm is to identify the category of a given dataset, and these algorithms are mainly used to predict the output for the categorical data. 
Classification algorithms can be better understood using the below diagram. In the below diagram, there are two classes, class A and Class B. These classes have features that are similar to each other and dissimilar to other classes. 
 
The algorithm which implements the classification on a dataset is known as a classifier. There are two types of Classifications:
	Binary Classifier: If the classification problem has only two possible outcomes, then it is called as Binary Classifier.
Examples: YES or NO, MALE or FEMALE, SPAM or NOT SPAM, CAT or DOG, etc.
	Multi-class Classifier: If a classification problem has more than two outcomes, then it is called as Multi-class Classifier.
Example: Classifications of types of crops, Classification of types of music.
Learners in Classification Problems:
In the classification problems, there are two types of learners:
	Lazy Learners: Lazy Learner firstly stores the training dataset and wait until it receives the test dataset. In Lazy learner case, classification is done on the basis of the most related data stored in the training dataset. It takes less time in training but more time for predictions.
Example: K-NN algorithm, Case-based reasoning
	Eager Learners:Eager Learners develop a classification model based on a training dataset before receiving a test dataset. Opposite to Lazy learners, Eager Learner takes more time in learning, and less time in prediction. Example: Decision Trees, Naïve Bayes, ANN.
4.4.2 Types of ML Classification Algorithms:
Classification Algorithms can be further divided into two category:
	Linear Models 
	Logistic Regression
	Support Vector Machines
	Non-linear Models 
	K-Nearest Neighbours
	Kernel SVM
	Naïve Bayes
	Decision Tree Classification
	Random Forest Classification
	XGBoost Classification
4.4.3 Evaluating a Classification model:
Once our model is completed, it is necessary to evaluate its performance; either it is a Classification or Regression model. So for evaluating a Classification model, we have the following ways:
1. Confusion Matrix:
	The confusion matrix provides us a matrix/table as output and describes the performance of the model.
	It is also known as the error matrix.
	The matrix consists of predictions result in a summarized form, which has a total number of correct predictions and incorrect predictions. The matrix looks like as below table:
	Actual Positive	Actual Negative 
Predicted Positive	True Positive	False Positive
Predicted Negative	False Negative	True Negative

 
2. AUC-ROC curve:
	ROC curve stands for Receiver Operating Characteristics Curve and AUC stands for Area Under the Curve.
	It is a graph that shows the performance of the classification model at different thresholds. 
	To visualize the performance of the multi-class classification model, we use the AUC-ROC Curve.
	The ROC curve is plotted with TPR and FPR, where TPR (True Positive Rate) on Y-axis and FPR(False Positive Rate) on X-axis. 
5.5 Logistic Regression
Before going on with the logistic regression, let us understand what is linear regression and why logistic regression in spite of linear Regression. 

4.5.1 Linear Regression
Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc. 
Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.
The linear regression model provides a sloped straight line representing the relationship between the variables. Consider the below image:
 
Mathematically, we can represent a multiple linear regression as:
y= β0+β1x1+ β2x2 + β3x3 + … + ε
Here,
Y= Dependent Variable (Target Variable),
x1, x2, … = Independent Variables (predictor Variables),
β0= intercept of the line (Gives an additional degree of freedom),
βi= Linear regression coefficient (scale factor to each input value) for all i≥1,
ε = random error.

Assumptions Of linear Relationship
	Linear regression assumes the linear relationship between the dependent and independent variables.
	Small or no multicollinearity between the features.
	Normal distribution of error terms.
	No autocorrelations
Types of Linear Regression
Linear regression can be further divided into two types of the algorithm:
	Simple Linear Regression:
If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression. 
	Multiple Linear regression:
If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression. 
Model Performance:
The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is called optimization. It can be achieved by below method:
R-squared method:
	R-squared is a statistical method that determines the goodness of fit.
	It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%.
	The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.
	It is also called a coefficient of determination, or coefficient of multiple determination for multiple regression. 
	It can be calculated from the below formula:
 



4.5.2 Logistic Regression:
	Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.
	Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.
	Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.
	In Logistic regression, instead of fitting a regression line, we fit an "S" shaped logistic function, which predicts two maximum values (0 or 1).
	The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.
	Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.
	Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function: 
 
Logistic Function (Sigmoid Function):
	The sigmoid function is a mathematical function used to map the predicted values to probabilities. 
	It maps any real value into another value within a range of 0 and 1.
	The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the "S" form. The S-form curve is called the Sigmoid function or the logistic function.
	In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0. 

Assumptions for Logistic Regression:
	The dependent variable must be categorical in nature. 
	The independent variable should not have multicollinearity.
Logistic Regression Equation:
The Logistic regression equation can be obtained from the Linear Regression equation. The mathematical steps to get Logistic Regression equations are given below:
	We know the equation of the straight line can be written as:
y = β0 + β1x1+ β2x2 + β3x3 + … + βnxn
	In Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation by (1-y):
y/(1-y)   ;0 for y=0,and infinity for y=1
	But we need range between -[infinity] to +[infinity], then take logarithm of the equation it will become:
log⁡〖[ 〗  y/(1-y)  ]= β0+β1x1+ β2x2+β3x3+⋯+ βnxn
The above equation is the final equation for Logistic Regression.
Type of Logistic Regression:
On the basis of the categories, Logistic Regression can be classified into three types:
	Binomial: In binomial Logistic regression, there can be only two possible types of the dependent variables, such as 0 or 1, Pass or Fail, etc. 
	Multinomial: In multinomial Logistic regression, there can be 3 or more possible unordered types of the dependent variable, such as "cat", "dogs", or "sheep" 
	Ordinal: In ordinal Logistic regression, there can be 3 or more possible ordered types of dependent variables, such as "low", "Medium", or "High". 
4.5.3 Linear Regression v/s Logistic Regression
Linear Regression and Logistic Regression are the two famous Machine Learning Algorithms which come under supervised learning technique. Since both the algorithms are of supervised in nature hence these algorithms use labeled dataset to make the predictions. But the main difference between them is how they are being used. The Linear Regression is used for solving Regression problems whereas Logistic Regression is used for solving the Classification problems. The description of both the algorithms is given below along with difference table. 
 

Linear Regression	Logistic Regression
	Linear regression is used to predict the continuous dependent variable using a given set of independent variables.		Logistic Regression is used to predict the categorical dependent variable using a given set of independent variables.
	Linear Regression is used for solving Regression problem.		Logistic regression is used for solving Classification problems.
	In Linear regression, we predict the value of continuous variables.		In logistic Regression, we predict the values of categorical variables.
	In linear regression, we find the best fit line, by which we can easily predict the output.		In Logistic Regression, we find the S-curve by which we can classify the samples.
	Least square estimation method is used for estimation of accuracy.		Maximum likelihood estimation method is used for estimation of accuracy.
	The output for Linear Regression must be a continuous value, such as price, age, etc.		The output of Logistic Regression must be a Categorical value such as 0 or 1, Yes or No, etc.
	In Linear regression, it is required that relationship between dependent variable and independent variable must be linear.		In Logistic regression, it is not required to have the linear relationship between the dependent and independent variable.
	In linear regression, there may be collinearity between the independent variables.		In logistic regression, there should not be collinearity between the independent variable.





Input: Fit the logistic regression model for training data
model1=LogisticRegression()
model1.fit(X_train,y_train)
y_train_pred_log=model1.predict(X_train)
#interpreting the weights
rint('intercept ', model1.intercept_[0])

pd.DataFrame({'coeff': model1.coef_[0]}, 
             index=X.columns)
Output: 
intercept  -1.972963409915088

	coeff
gender	0.210021
age	0.598966
cigsPerDay	0.238324
BPMeds	0.020914
prevalentStroke	0.071004
prevalentHyp	0.289272
diabetes	0.108598
totChol	0.064502
BMI	0.091520
heartRate	0.015158

Input:
#Confusion Matrix
cm = confusion_matrix(y_train,y_train_pred_log)
conf_matrix = pd.DataFrame(cm, columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])

plt.figure(figsize=(5,5))
sns.heatmap(conf_matrix,fmt='d',cmap='Blues',annot=True)
plt.show()

print('The details of Confusion_matrix is:')
print(classification_report(y_train,y_train_pred_log))

print('Accuracy of Logistic Regression Model:',accuracy_score(y_train_pred_log,y_train))

Output for training data:
 
Conclusion: From the above output, it is clear that the logistic regression model has predicted correctly for 2119 observation and the remaining 360 observations are being misclassified. Thus, the accuracy of this model for training dataset is 85.4%.

Input: Predict for test data
y_test_pred_log=model1.predict(X_test)
#Confusion Matrix
cm = confusion_matrix(y_test,y_test_pred_log)
conf_matrix = pd.DataFrame(cm, columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])

plt.figure(figsize=(5,5))
sns.heatmap(conf_matrix,fmt='d',cmap='Blues',annot=True)
plt.show()

print('The details of Confusion_matrix is:')
print(classification_report(y_test,y_test_pred_log))
print('Accuracy of Logistic Regression Model:',accuracy_score(y_test_pred_log,y_test))
Output:
 
Conclusion: From the above output, it is clear that the logistic regression model has predicted correctly for 1410 observation and the remaining 453 observations are being misclassified. Thus, the accuracy of this model for training dataset is 84.7%.
4.6 Ensemble
We all use the Decision Tree Technique on day to day life to make the decision. Organizations use these supervised machine learning techniques like Decision trees to make a better decision and to generate more surplus and profit.
Ensemble methods combine different decision trees to deliver better predictive results, afterward utilizing a single decision tree. The primary principle behind the ensemble model is that a group of weak learners come together to form an active learner.
There are two techniques given below that are used to perform ensemble decision tree.
 
4.6.1 Bagging
Bagging is used when our objective is to reduce the variance of a decision tree. Here the concept is to create a few subsets of data from the training sample, which is chosen randomly with replacement. Now each collection of subset data is used to prepare their decision trees thus, we end up with an ensemble of various models. The average of all the assumptions from numerous tress is used, which is more powerful than a single decision tree.
Random Forest is an expansion over bagging. It takes one additional step to predict a random subset of data. It also makes the random selection of features rather than using all features to develop trees. When we have numerous random trees, it is called the Random Forest.
These are the following steps which are taken to implement a Random forest:
	Let us consider X observations Y features in the training data set. First, a model from the training data set is taken randomly with substitution.
	The tree is developed to the largest.
	The given steps are repeated, and prediction is given, which is based on the collection of predictions from n number of trees.
Advantages of using Random Forest technique:
	It manages a higher dimension data set very well.
	It manages missing quantities and keeps accuracy for missing data.
Disadvantages of using Random Forest technique:
Since the last prediction depends on the mean predictions from subset trees, it won't give precise value for the regression model.
4.6.2 Boosting:
Boosting is another ensemble procedure to make a collection of predictors. In other words, we fit consecutive trees, usually random samples, and at each step, the objective is to solve net error from the prior trees.
If a given input is misclassified by theory, then its weight is increased so that the upcoming hypothesis is more likely to classify it correctly by consolidating the entire set at last converts weak learners into better performing models.
Gradient Boosting is an expansion of the boosting procedure.
	Gradient Boosting = Gradient Descent + Boosting  
It utilizes a gradient descent algorithm that can optimize any differentiable loss function. An ensemble of trees is constructed individually, and individual trees are summed successively. The next tree tries to restore the loss (It is the difference between actual and predicted values).
Gradient boosted decision trees are implemented by the XGBoost library of Python, intended for speed and execution, which is the most important aspect of ML (machine learning).
Advantages of using Gradient Boosting methods:
	It supports different loss functions.
	It works well with interactions.
Disadvantages of using a Gradient Boosting methods:
	It requires cautious tuning of different hyper-parameters.








Difference between Bagging and Boosting:
Bagging	Boosting
Various training data subsets are randomly drawn with replacement from the whole training dataset.	Each new subset contains the components that were misclassified by previous models.
Bagging attempts to tackle the over-fitting issue.	Boosting tries to reduce bias.
If the classifier is unstable (high variance), then we need to apply bagging.	If the classifier is steady and straightforward (high bias), then we need to apply boosting.
Every model receives an equal weight.	Models are weighted by their performance.
Objective to decrease variance, not bias.	Objective to decrease bias, not variance.
It is the easiest way of connecting predictions that belong to the same type.	It is a way of connecting predictions that belong to the different types.
Every model is constructed independently.	New models are affected by the performance of the previously developed model.
________________________________________


Input for training dataset:
#RandomForest
model3 = RandomForestClassifier()
model3.fit(X_train,y_train)
y_train_pred_rf = model3.predict(X_train)
#Confusion matrix
cm = confusion_matrix(y_train,y_train_pred_rf)
conf_matrix=pd.DataFrame(data=cm, columns=["Predict:0","Predict:1"],index=["Actual:0","Actual:1"])

plt.figure(figsize=(5,5))
sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Reds')
plt.show()

print("The details of Confusion matrix:")
print(classification_report(y_train,y_train_pred_rf))

print("Accuracy Score of RandomForest:",accuracy_score(y_train_pred_rf,y_train))



Output:
 
Conclusion: From the above output, it is clear that the Random Forest model has predicted all the observations. Thus, the accuracy of this model for training dataset is 100%.
Input for test dataset:
y_test_pred_rf = model3.predict(X_test)
cm = confusion_matrix(y_test,y_test_pred_rf)
conf_matrix=pd.DataFrame(data=cm, columns=["Predict:0","Predict:1"],index=["Actual:0","Actual:1"])

plt.figure(figsize=(5,5))
sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Reds')
plt.show()

print("The details of Confusion matrix:")
print(classification_report(y_test,y_test_pred_rf))
print("Accuracy Score of randomForest:",accuracy_score(y_test_pred_rf,y_test))


















Output:
 

Conclusion: From the above output, it is clear that the Random Forest model has predicted correctly for 1382 observation and the remaining 272 observations are being misclassified. Thus, the accuracy of this model for training dataset is 83.5%.

Input for training dataset:
#XGBoost
model2=xg.XGBClassifier()
model2.fit(X_train,y_train)
y_train_pred_xg=model2.predict(X_train)
cm = confusion_matrix(y_train,y_train_pred_xg)
conf_matrix=pd.DataFrame(data=cm, columns=["Predict:0","Predict:1"],index=["Actual:0","Actual:1"])

plt.figure(figsize=(5,5))
sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Reds')
plt.show()

print("The details of Confusion matrix:")
print(classification_report(y_train,y_train_pred_xg))
print("Accuracy Score of XGBoost:",accuracy_score(y_train_pred_xg,y_train))
 
Conclusion: From the above output, it is clear that the XGBoost model has predicted correctly for 2432 observation and the remaining 47 observations are being misclassified. Thus, the accuracy of this model for training dataset is 98.1%.



Input for test data:
#XGBoost
y_test_pred_xg = model2.predict(X_test)
cm = confusion_matrix(y_test,y_test_pred_xg)
conf_matrix=pd.DataFrame(data=cm, columns=["Predict:0","Predict:1"],index=["Actual:0","Actual:1"])

plt.figure(figsize=(5,5))
sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Reds')
plt.show()

print("The details of Confusion matrix:")
print(classification_report(y_test,y_test_pred_xg))

print("Accuracy Score of XGBoost:",accuracy_score(y_test_pred_xg,y_test))


















Output:
 
Conclusion: From the above output, it is clear that the XGBoost model has predicted correctly for 1367 observation and the remaining 287 observations are being misclassified. Thus, the accuracy of this model for training dataset is 82.6%.

4.6.3 AUC-ROC Curve

In Machine Learning, only developing an ML model is not sufficient as we also need to see whether it is performing well or not. It means that after building an ML model, we need to evaluate and validate how good or bad it is, and for such cases, we use different Evaluation Metrics. AUC-ROC curve is such an evaluation metric that is used to visualize the performance of a classification model. It is one of the popular and important metrics for evaluating the performance of the classification model. In this topic, we are going to discuss more details about the AUC-ROC curve.
What is AUC-ROC Curve?
AUC-ROC curve is a performance measurement metric of a classification model at different threshold values. Firstly, let's understand ROC (Receiver Operating Characteristic curve) curve.
ROC Curve
ROC or Receiver Operating Characteristic curve represents a probability graph to show the performance of a classification model at different threshold levels. The curve is plotted between two parameters, which are:
	True Positive Rate or TPR
	False Positive Rate or FPR
In the curve, TPR is plotted on Y-axis, whereas FPR is on the X-axis.
TPR:
TPR or True Positive rate is a synonym for Recall, which can be calculated as:
 
FPR or False Positive Rate can be calculated as:
 
Here, TP: True Positive
FP: False Positive
TN: True Negative
FN: False Negative
Now, to efficiently calculate the values at any threshold level, we need a method, which is AUC.

AUC: Area Under the ROC curve
AUC is known for Area Under the ROC curve. As its name suggests, AUC calculates the two-dimensional area under the entire ROC curve ranging from (0,0) to (1,1), as shown below image:
 
In the ROC curve, AUC computes the performance of the binary classifier across different thresholds and provides an aggregate measure. The value of AUC ranges from 0 to 1, which means an excellent model will have AUC near 1, and hence it will show a good measure of Separability.
When to Use AUC-ROC
AUC is preferred due to the following cases:
	AUC is used to measure how well the predictions are ranked instead of giving their absolute values. Hence, we can say AUC is Scale-Invariant.
	It measures the quality of predictions of the model without considering the selected classification threshold. It means AUC is classification-threshold-invariant.
When not to use AUC-ROC
	AUC is not preferable when we need to calibrate probability output.
	Further, AUC is not a useful metric when there are wide disparities in the cost of false negatives v/s false positives, and it is difficult to minimize one type of classification error.
How AUC-ROC curve can be used for the Multi-class Model?
Although the AUC-ROC curve is only used for binary classification problems, we can also use it for multiclass classification problems. For multi-class classification problems, we can plot N number of AUC curves for N number of classes with the One vs ALL method.
For example, if we have three different classes, X, Y, and Z, then we can plot a curve for X against Y & Z, a second plot for Y against X & Z, and the third plot for Z against Y and X.


Applications of AUC-ROC Curve
Although the AUC-ROC curve is used to evaluate a classification model, it is widely used for various applications. Some of the important applications of AUC-ROC are given below:
	Classification of 3D model
The curve is used to classify a 3D model and separate it from the normal models. With the specified threshold level, the curve classifies the non-3D and separates out the 3D models.
	Healthcare
The curve has various applications in the healthcare sector. It can be used to detect cancer disease in patients. It does this by using false positive and false negative rates, and accuracy depends on the threshold value used for the curve.
	Binary Classification
AUC-ROC curve is mainly used for binary classification problems to evaluate their performance.



Input for training dataset: 
#Plot an roc curve
fpr, tpr, _ = metrics.roc_curve(y_train,y_train_pred_log)
auc = round(metrics.roc_auc_score(y_train,y_train_pred_log), 4)
plt.plot(fpr,tpr,label='Logistic Regression, AUC='+str(auc))

fpr, tpr, _ = metrics.roc_curve(y_train,y_train_pred_xg)
auc = round(metrics.roc_auc_score(y_train,y_train_pred_xg),4)
plt.plot(fpr,tpr,label='XGBoost, AUC='+str(auc))

fpr, tpr, _ = metrics.roc_curve(y_train,y_train_pred_rf)
auc = round(metrics.roc_auc_score(y_train,y_train_pred_rf),4)
plt.plot(fpr,tpr,label='RandomForest, AUC='+str(auc))

plt.legend()
plt.show()

Output for training data:
 
Conclusion: From the above ROC plot, it is clear that the Random Forest has greater AUC than the remaining models.

Input for test data:
#Plor an roc curve
fpr, tpr, _ = metrics.roc_curve(y_test,y_test_pred_log)
auc = round(metrics.roc_auc_score(y_test,y_test_pred_log), 4)
plt.plot(fpr,tpr,label='Logistic Regression, AUC='+str(auc))

fpr, tpr, _ = metrics.roc_curve(y_test,y_test_pred_xg)
auc = round(metrics.roc_auc_score(y_test,y_test_pred_xg),4)
plt.plot(fpr,tpr,label='XGBoost, AUC='+str(auc))

fpr, tpr, _ = metrics.roc_curve(y_test,y_test_pred_rf)
auc = round(metrics.roc_auc_score(y_test,y_test_pred_rf),4)
plt.plot(fpr,tpr,label='RandomForest, AUC='+str(auc))

plt.legend()
plt.show()
	

Output:
 
Conclusion: From the above ROC plot, it is clear that the XGBoost has greater AUC than the remaining models.









5.0 Summary:
In present days, heart disease and stroke has become the top killers among both women and men. Cardiovascular diseases such as ischaemic heart disease and cerebrovascular such as stroke account for 17.7 million deaths and are the leading cause. In accordance with the World Health Organization, India accounts for one-fifth of these deaths worldwide especially in younger population. 
For building a Machine Learning model, the data needs to be preprocessed. The preprocessing of the data includes checking for the presence of the missing observations, outlier detection and feature scaling.  Once the preprocessing is done, we are going to split the data into training and testing. In this case 60% of the data is used for training the model and 40% of the data is used to test the model. We will be using three algorithms i.e., Logistic Regression, Random Forest and XGBoost as they are not sensitive towards the outliers. From the use of confusion matrix and accuracy score, we say that Logistic Regression gives 85.4% accuracy for training data and 84.7% for test data. Whereas, XGBoost gives 98.1% accuracy for training data and 82.6% for test data. Similarly, the accuracy of Random Forest for training data is 99.9% and 84% for test data.  Thus, AUC and ROC curve is used to analyse which of the model best suits in this case. According to AUC and ROC curve Random Forest suits well for training data as it has the highest AUC (i.e., 0.9986) as compaired to other two models (i.e., AUC of Logistic Regression = 0.5187 and AUC of XGBoost = 0.9361). On the other hand, XGBoost suits well for the test data (according to ROC and AUC curve) as it has the highest AUC (i.e., 0.5232) when compaired with the other two models (i.e., AUC of Logistic Regression = 0.5197 and AUC of Random Forest = 0.5177).
